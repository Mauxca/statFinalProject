\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{grffile}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{graphicx}
%% begin.rcode setup, include=FALSE
% opts_knit$set(width=80)
% library(knitr)
% opts_chunk$set(fig.path='figure/latex-', cache.path='cache/latex-')
%% end.rcode

\begin{document} 

\title{Homework 10}
\author{Student: Li Duo}
\date{2016-04-13}
\maketitle




\section*{1}
\subsection*{a}


%%begin.rcode
load("/Users/PAT/Downloads/HW10.rda")

n=nrow(data)
p=ncol(data)-1

GetRSS  = function(y, yhat) sum((y - yhat)^2)
GetMSE  = function(y, yhat) sum((y - yhat)^2)/length(y)
GetRsquared = function(y, yhat) 1 - GetRSS(y, yhat)/sum((y - mean(y))^2)

model_ols<-lm(Y~.,data=data)
summary(model_ols)


%%end.rcode

\subsection*{b}
%%begin.rcode
r2_ols<-GetRsquared(data$Y,model_ols$fitted.values)
r2_ols
%%end.rcode

\subsection*{c}
%%begin.rcode
set.seed(1)
n = nrow(data)
n_fold = 10
### Create fold for Cross Validation
fold_id = c()
for (i in 1:n_fold) {
        fold_id = c(fold_id, rep(i, as.integer(n / n_fold)))
}
fold_id = c(fold_id, rep(1, n - length(fold_id))) # Incase there are extra obs
fold_id = sample(fold_id, size = n, replace = FALSE)


r2_ols_cv<-rep(0,n_fold)
for (i in 1:n_fold) {
        model_ols_cv = lm(Y~.,data=data[fold_id!=i,])
        yhat = predict(model_ols_cv, newdata = data[fold_id == i,-1])
        actual_y=data$Y[fold_id==i]
        r2_ols_cv[i]<-GetRsquared(actual_y,yhat)
}
mean(r2_ols_cv)

%%end.rcode
Comment: Multiple $R^2$is 0.50 while adjusted $R^2$ is about 0.37. The cross-validated $R^2$ is closer to the adjusted $R^2$, which is relatively lower.

\section*{2}
%%begin.rcode
y<-data$Y
x<-data[,-1]
predictors<-data[,-1]
drop_out_index<-NULL


train_errors<-rep(0,20)
test_errors<-rep(0,20)
for (p in 1:20){
model<-lm(y~as.matrix(predictors))
train_errors[p]<-GetMSE(y,model$fitted.values)

test_error<-rep(0,10)
for (i in 1:10){
if (is.null(dim(predictors))==F){        
train<-data.frame(y=y[fold_id!=i],x=predictors[fold_id!=i,])
test_y<-y[fold_id==i]
test_x<-predictors[fold_id==i,]}
else{train<-data.frame(y=y[fold_id!=i],x=predictors[fold_id!=i])
     test_y<-y[fold_id==i]
     test_x<-predictors[fold_id==i]
        
}

model<-lm(y~.,data=train)

yhat<-predict(model,newdata = data.frame(x=test_x))

test_error[i]<-GetMSE(test_y,yhat)
}

test_errors[p]<-mean(test_error)
if (is.null(dim(predictors))==F){
tvalues<-coef(summary(model))[,3]
min_t<-min(abs(tvalues[-1]))
drop_out_index<-which(abs(tvalues)==abs(min_t))-1

print(names(predictors)[-drop_out_index])
predictors<-predictors[,-drop_out_index]

}
else{break}
}
test_errors<-rev(test_errors)
train_errors<-rev(train_errors)


%%end.rcode


\section*{3}

%%begin.rcode
y<-data$Y
x<-data[,-1]
predictors=NULL
pool<-x

train_errors_2<-rep(0,20)
test_errors_2<-rep(0,20)
for (t in 1:20){
if (is.null(dim(pool))){
        times<-1
}else{
        times<-ncol(pool)
}        
        
rss_set<-rep(0,times)
for (p in 1:times){
        if (!is.null(dim(pool))){
        new_predictors<-cbind(predictors,pool[,p])
        }else {
                new_predictors<-cbind(predictors,pool)
        }
        model_data<-data.frame(y=y,x=new_predictors)
        model<-lm(y~.,model_data)
        #model<-lm((y[fold_id!=i])~predictors[(fold_id!=i),])
        #yhat<-predict(model,newdata = predictors[fold_id==i,])
        rss_set[p]<-sum(model$residuals^2)
        }
rss_min<-min(rss_set)
train_errors_2[t]<-rss_min/length(y)
predictor_index<-which(rss_set==rss_min)
if (is.null(dim(pool))){
        break
}else{
 
predictors<-cbind(predictors,pool[,predictor_index])

pool<-pool[,-predictor_index]}
var_names<-names(x)[!names(x)%in%names(pool)]
print(var_names)    

for (i in 1:10){
                if (is.null(dim(predictors))==F){        
                        train<-data.frame(y=y[fold_id!=i],x=predictors[fold_id!=i,])
                        test_y<-y[fold_id==i]
                        test_x<-predictors[fold_id==i,]}
                else{train<-data.frame(y=y[fold_id!=i],x=predictors[fold_id!=i])
                     test_y<-y[fold_id==i]
                     test_x<-predictors[fold_id==i]
                     
                }
                
                model<-lm(y~.,data=train)
                
                yhat<-predict(model,newdata = data.frame(x=test_x))
                
                test_error[i]<-GetMSE(test_y,yhat)
        }
        
        test_errors_2[t]<-mean(test_error)
}
#add the full model
model<-lm(Y~.,data=data)
train_errors_2[20]<-GetMSE(data$Y,model$fitted.values)

for (i in 1:10){
              train<-data.frame(y=y[fold_id!=i],x=x[fold_id!=i,])
             test_y<-y[fold_id==i]
             test_x<-x[fold_id==i,]
        
        model<-lm(y~.,data=train)
        
        yhat<-predict(model,newdata = data.frame(x=test_x))
        
        test_error[i]<-GetMSE(test_y,yhat)
}

test_errors_2[20]<-mean(test_error)

%%end.rcode
Comments: They do not have exactly the same sequence. This may due to some random errors but I would argue that it 
may be due to we used different criteria when selecting model. (one is t statistics and another one RSS)

\section*{4}

%%begin.rcode

plot(train_errors,col='blue',type='l',lty='solid',ylab = 'errors',xlab='#of predictors')
lines(test_errors,col='red',lty='solid')
lines(train_errors_2,col='blue',lty='dashed')
lines(test_errors_2,col='red',lty='dashed')
abline(v=which(train_errors==min(train_errors)),col='blue',lty='solid')
abline(v=which(test_errors==min(test_errors)),col='red',lty='solid')
abline(v=which(train_errors_2==min(train_errors_2)),col='blue',lty='dashed')
abline(v=which(test_errors_2==min(test_errors_2)),col='red',lty='dashed')


legend(0.1,1.1,legend=c('Backward training erros','Backward test erros','Forward training erros','Forward test erros'),col=c('blue','red','blue','red'),lty=c('solid','solid','dashed','dashed'),cex=0.8)


%%end.rcode

Comment: Backforwad and forward model selection have similar performance. 
For training errors, they always decrease when more variables are added. The best model chosen would always be the one with most number of variables.
For training errors, they decrease first while then increase (overfitting). The best model chosen would be somewhere in between, taking the penality 
of extra number of variables into account.






\end{document}
